{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "This notebook should include preliminary and baseline modeling.\n",
    "- Try as many different models as possible.\n",
    "- Don't worry about hyperparameter tuning or cross validation here.\n",
    "- Ideas include:\n",
    "    - linear regression\n",
    "    - support vector machines\n",
    "    - random forest\n",
    "    - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from the pickle file\n",
    "# data = pd.read_pickle('data.pkl')\n",
    "# print(\"DataFrame loaded successfully\")\n",
    "# df = data\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled data\n",
    "X_train = pd.read_pickle('pickles/X_train.pkl')\n",
    "X_test = pd.read_pickle('pickles/X_test.pkl')\n",
    "y_train = pd.read_pickle('pickles/y_train.pkl')\n",
    "y_test = pd.read_pickle('pickles/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in X_train:\n",
      "descrip_year_built             0\n",
      "descrip_sold_date              0\n",
      "descrip_lot_sqft               0\n",
      "descrip_sqft                   0\n",
      "garage_car_spaces              0\n",
      "descrip_stories                0\n",
      "descrip_beds                   0\n",
      "descrip_baths                  0\n",
      "property_type_condo            0\n",
      "property_type_land             0\n",
      "property_type_mobile           0\n",
      "property_type_multi_family     0\n",
      "property_type_other_unknown    0\n",
      "property_type_single_family    0\n",
      "property_type_townhouse        0\n",
      "has_parking                    0\n",
      "heating                        0\n",
      "porch                          0\n",
      "laundry_facilities             0\n",
      "notable_kitchen                0\n",
      "notable_ceiling                0\n",
      "recently_updated               0\n",
      "multimedia_room                0\n",
      "enclosed_yard                  0\n",
      "cul_de_sac                     0\n",
      "family_room                    0\n",
      "hardwood_floors                0\n",
      "basement                       0\n",
      "rental_property                0\n",
      "furniture                      0\n",
      "private_courtyard              0\n",
      "central_air                    0\n",
      "dishwasher                     0\n",
      "fireplace                      0\n",
      "den_or_office                  0\n",
      "smart_homes                    0\n",
      "guest_parking                  0\n",
      "storm_shelter                  0\n",
      "exposed_brick                  0\n",
      "solar_power                    0\n",
      "security                       0\n",
      "pets_allowed                   0\n",
      "dining_room                    0\n",
      "wine_cellar                    0\n",
      "state_target_encoded           0\n",
      "city_target_encoded            0\n",
      "dtype: int64\n",
      "Null values in X_test:\n",
      "descrip_year_built             0\n",
      "descrip_sold_date              0\n",
      "descrip_lot_sqft               0\n",
      "descrip_sqft                   0\n",
      "garage_car_spaces              0\n",
      "descrip_stories                0\n",
      "descrip_beds                   0\n",
      "descrip_baths                  0\n",
      "property_type_condo            0\n",
      "property_type_land             0\n",
      "property_type_mobile           0\n",
      "property_type_multi_family     0\n",
      "property_type_other_unknown    0\n",
      "property_type_single_family    0\n",
      "property_type_townhouse        0\n",
      "has_parking                    0\n",
      "heating                        0\n",
      "porch                          0\n",
      "laundry_facilities             0\n",
      "notable_kitchen                0\n",
      "notable_ceiling                0\n",
      "recently_updated               0\n",
      "multimedia_room                0\n",
      "enclosed_yard                  0\n",
      "cul_de_sac                     0\n",
      "family_room                    0\n",
      "hardwood_floors                0\n",
      "basement                       0\n",
      "rental_property                0\n",
      "furniture                      0\n",
      "private_courtyard              0\n",
      "central_air                    0\n",
      "dishwasher                     0\n",
      "fireplace                      0\n",
      "den_or_office                  0\n",
      "smart_homes                    0\n",
      "guest_parking                  0\n",
      "storm_shelter                  0\n",
      "exposed_brick                  0\n",
      "solar_power                    0\n",
      "security                       0\n",
      "pets_allowed                   0\n",
      "dining_room                    0\n",
      "wine_cellar                    0\n",
      "state_target_encoded           0\n",
      "city_target_encoded            8\n",
      "dtype: int64\n",
      "Null values in y_train:\n",
      "0\n",
      "Null values in y_test:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print(\"Null values in X_train:\")\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "print(\"Null values in X_test:\")\n",
    "print(X_test.isnull().sum())\n",
    "\n",
    "print(\"Null values in y_train:\")\n",
    "print(y_train.isnull().sum())\n",
    "\n",
    "print(\"Null values in y_test:\")\n",
    "print(y_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in X_test after dropping rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with null values in 'city_target_encoded' in X_test\n",
    "X_test = X_test.dropna(subset=['city_target_encoded'])\n",
    "\n",
    "# Verify that null values in 'city_target_encoded' are removed\n",
    "print(\"Null values in X_test after dropping rows:\")\n",
    "print(X_test['city_target_encoded'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDTypePromotionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 2\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m      3\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\Users\\mebar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mebar\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\mebar\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\mebar\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mebar\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    913\u001b[0m     X,\n\u001b[0;32m    914\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    915\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    916\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    917\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    918\u001b[0m )\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mebar\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\mebar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    875\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    876\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[0;32m    877\u001b[0m )\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 879\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdtypes_orig)\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[1;31mDTypePromotionError\u001b[0m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.isnull of       descrip_year_built  descrip_lot_sqft  descrip_sqft  garage_car_spaces  \\\n",
       "323               2005.0            4497.0        1848.0                2.0   \n",
       "409               1987.0            3206.0        3206.0                0.0   \n",
       "406               1920.0            4356.0        1086.0                0.0   \n",
       "463               1961.0           13068.0         875.0                0.0   \n",
       "1141              1971.0            6534.0        1056.0                2.0   \n",
       "...                  ...               ...           ...                ...   \n",
       "1645              1956.0            5635.0        1425.0                2.0   \n",
       "710               1910.0            9583.0        1055.0                0.0   \n",
       "440               1948.0            8581.0         864.0                1.0   \n",
       "1239              1975.0            7841.0        1634.0                0.0   \n",
       "955               1947.0            6970.0         676.0                0.0   \n",
       "\n",
       "      descrip_stories  descrip_beds  descrip_baths  property_type_condo  \\\n",
       "323               2.0           3.0            2.0                  0.0   \n",
       "409               1.0           3.0            3.0                  1.0   \n",
       "406               1.0           2.0            1.0                  0.0   \n",
       "463               0.0           3.0            1.0                  0.0   \n",
       "1141              1.0           3.0            2.0                  0.0   \n",
       "...               ...           ...            ...                  ...   \n",
       "1645              1.0           3.0            2.0                  0.0   \n",
       "710               1.0           2.0            1.0                  1.0   \n",
       "440               0.0           2.0            2.0                  0.0   \n",
       "1239              0.0           0.0            0.0                  1.0   \n",
       "955               1.0           2.0            1.0                  0.0   \n",
       "\n",
       "      property_type_land  property_type_mobile  ...  storm_shelter  \\\n",
       "323                  0.0                   0.0  ...            0.0   \n",
       "409                  0.0                   0.0  ...            0.0   \n",
       "406                  0.0                   0.0  ...            0.0   \n",
       "463                  0.0                   0.0  ...            0.0   \n",
       "1141                 0.0                   0.0  ...            0.0   \n",
       "...                  ...                   ...  ...            ...   \n",
       "1645                 0.0                   0.0  ...            0.0   \n",
       "710                  0.0                   0.0  ...            0.0   \n",
       "440                  0.0                   0.0  ...            0.0   \n",
       "1239                 0.0                   0.0  ...            0.0   \n",
       "955                  0.0                   0.0  ...            0.0   \n",
       "\n",
       "      exposed_brick  solar_power  security  pets_allowed  dining_room  \\\n",
       "323             0.0          0.0       0.0           0.0          0.0   \n",
       "409             0.0          0.0       0.0           0.0          0.0   \n",
       "406             0.0          0.0       0.0           0.0          0.0   \n",
       "463             0.0          0.0       0.0           0.0          0.0   \n",
       "1141            0.0          0.0       0.0           0.0          0.0   \n",
       "...             ...          ...       ...           ...          ...   \n",
       "1645            0.0          0.0       0.0           0.0          0.0   \n",
       "710             0.0          0.0       0.0           1.0          1.0   \n",
       "440             0.0          0.0       0.0           0.0          0.0   \n",
       "1239            0.0          0.0       0.0           0.0          0.0   \n",
       "955             0.0          0.0       0.0           0.0          0.0   \n",
       "\n",
       "      wine_cellar  days_since_sold  state_target_encoded  city_target_encoded  \n",
       "323           0.0              346          3.270593e+05         3.270593e+05  \n",
       "409           1.0              304          5.268300e+05         4.924017e+05  \n",
       "406           0.0              304          5.268300e+05         4.924017e+05  \n",
       "463           0.0              307          3.230026e+05         2.175966e+05  \n",
       "1141          0.0              321          5.035927e+05         5.035927e+05  \n",
       "...           ...              ...                   ...                  ...  \n",
       "1645          0.0              307          4.426038e+05         4.423566e+05  \n",
       "710           0.0              301          1.062229e+06         1.062229e+06  \n",
       "440           0.0              303          3.230026e+05         2.175966e+05  \n",
       "1239          0.0              406          4.025129e+05         3.844333e+05  \n",
       "955           0.0              301          6.450823e+05         6.450823e+05  \n",
       "\n",
       "[343 rows x 46 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1404, 46)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199      480000.0\n",
       "1382     260000.0\n",
       "801      185000.0\n",
       "1240     348900.0\n",
       "1594     524950.0\n",
       "          ...    \n",
       "1134     689900.0\n",
       "1299      95000.0\n",
       "863      250000.0\n",
       "1464     335000.0\n",
       "1130    1500000.0\n",
       "Name: descrip_sold_price, Length: 1404, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1404,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 145817530500.2917\n",
      "R-squared: 0.5065117308530277\n"
     ]
    }
   ],
   "source": [
    "# Align y_test with X_test after dropping rows with nulls in 'city_target_encoded'\n",
    "y_test = y_test[X_test.index]\n",
    "\n",
    "# Initialize and fit the linear regression model\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_scaled, y_train.values)\n",
    "\n",
    "# Predict on the scaled test data\n",
    "y_pred_lr = linear_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred_lr)\n",
    "r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 301342955363.1485\n",
      "R-squared: -0.019830832078835847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize the Support Vector Regressor\n",
    "svm_regressor = SVR(kernel='rbf', C=100, epsilon=0.1)  # Will tune later\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_regressor.fit(X_train_scaled, y_train.values)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_svm = svm_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred_svm)\n",
    "r2 = r2_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 118289368284.31741\n",
      "R-squared: 0.5996749127979519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)  # Will tune later\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_regressor.fit(X_train_scaled, y_train.values)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_rf = rf_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred_rf)\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 152373904869.29163\n",
      "R-squared: 0.48432308297141446\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the XGBRegressor\n",
    "xgb_regressor = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_regressor.fit(X_train_scaled, y_train.values)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_xgb = xgb_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2 = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what metrics you want to use to evaluate success.\n",
    "- If you think about mean squared error, can we actually relate to the amount of error?\n",
    "- Try root mean squared error so that error is closer to the original units (dollars)\n",
    "- What does RMSE do to outliers?\n",
    "- Is mean absolute error a good metric for this problem?\n",
    "- What about R^2? Adjusted R^2?\n",
    "- Briefly describe your reasons for picking the metrics you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather evaluation metrics and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Metrics:\n",
      "Mean Squared Error (MSE): 145817530500.2917\n",
      "Root Mean Squared Error (RMSE): 381860.61658711504\n",
      "Mean Absolute Error (MAE): 169440.57358138394\n",
      "R-squared (R²): 0.5065117308530277\n",
      "Adjusted R-squared: 0.4298209863234306\n",
      "\n",
      "Support Vector Machine Metrics:\n",
      "Mean Squared Error (MSE): 301342955363.1485\n",
      "Root Mean Squared Error (RMSE): 548947.1334865941\n",
      "Mean Absolute Error (MAE): 240273.54469297137\n",
      "R-squared (R²): -0.019830832078835847\n",
      "Adjusted R-squared: -0.17831805598297934\n",
      "\n",
      "Random Forest Metrics:\n",
      "Mean Squared Error (MSE): 118289368284.31741\n",
      "Root Mean Squared Error (RMSE): 343932.21466492116\n",
      "Mean Absolute Error (MAE): 132034.9503377065\n",
      "R-squared (R²): 0.5996749127979519\n",
      "Adjusted R-squared: 0.5374622303273633\n",
      "\n",
      "XGBoost Metrics:\n",
      "Mean Squared Error (MSE): 152373904869.29163\n",
      "Root Mean Squared Error (RMSE): 390351.0021369122\n",
      "Mean Absolute Error (MAE): 141338.50884600493\n",
      "R-squared (R²): 0.48432308297141446\n",
      "Adjusted R-squared: 0.4041841026223775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to calculate metrics\n",
    "def calculate_metrics(y_test, y_pred, n, k):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "    \n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"R-squared (R²): {r2}\")\n",
    "    print(f\"Adjusted R-squared: {adj_r2}\")\n",
    "\n",
    "# Number of samples (n) and number of features (k)\n",
    "n = len(y_test)\n",
    "k = X_train.shape[1]\n",
    "\n",
    "# Linear Regression\n",
    "print(\"Linear Regression Metrics:\")\n",
    "calculate_metrics(y_test, y_pred_lr, n, k)\n",
    "\n",
    "# Support Vector Machine\n",
    "print(\"\\nSupport Vector Machine Metrics:\")\n",
    "calculate_metrics(y_test, y_pred_svm, n, k)\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\nRandom Forest Metrics:\")\n",
    "calculate_metrics(y_test, y_pred_rf, n, k)\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\nXGBoost Metrics:\")\n",
    "calculate_metrics(y_test, y_pred_xgb, n, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - STRETCH\n",
    "\n",
    "> **This step doesn't need to be part of your Minimum Viable Product (MVP), but its recommended you complete it if you have time!**\n",
    "\n",
    "Even with all the preprocessing we did in Notebook 1, you probably still have a lot of features. Are they all important for prediction?\n",
    "\n",
    "Investigate some feature selection algorithms (Lasso, RFE, Forward/Backward Selection)\n",
    "- Perform feature selection to get a reduced subset of your original features\n",
    "- Refit your models with this reduced dimensionality - how does performance change on your chosen metrics?\n",
    "- Based on this, should you include feature selection in your final pipeline? Explain\n",
    "\n",
    "Remember, feature selection often doesn't directly improve performance, but if performance remains the same, a simpler model is often preferrable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection \n",
    "# refit models\n",
    "# gather evaluation metrics and compare to the previous step (full feature set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Selected Features: Index(['descrip_year_built', 'descrip_lot_sqft', 'descrip_sqft',\n",
      "       'garage_car_spaces', 'descrip_stories', 'descrip_beds', 'descrip_baths',\n",
      "       'property_type_condo', 'property_type_land', 'property_type_mobile',\n",
      "       'property_type_multi_family', 'property_type_other_unknown',\n",
      "       'property_type_single_family', 'property_type_townhouse', 'has_parking',\n",
      "       'heating', 'porch', 'laundry_facilities', 'notable_kitchen',\n",
      "       'notable_ceiling', 'recently_updated', 'multimedia_room',\n",
      "       'enclosed_yard', 'cul_de_sac', 'family_room', 'hardwood_floors',\n",
      "       'basement', 'rental_property', 'furniture', 'private_courtyard',\n",
      "       'central_air', 'dishwasher', 'fireplace', 'den_or_office',\n",
      "       'smart_homes', 'guest_parking', 'storm_shelter', 'exposed_brick',\n",
      "       'solar_power', 'security', 'pets_allowed', 'dining_room', 'wine_cellar',\n",
      "       'days_since_sold', 'state_target_encoded', 'city_target_encoded'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahmu\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.600e+10, tolerance: 2.378e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure the data is scaled\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit Lasso\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)  # Adjust alpha as needed\n",
    "lasso.fit(X_train_scaled, y_train.values)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X_train.columns[(lasso.coef_ != 0)]\n",
    "print(\"Lasso Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE Selected Features: Index(['descrip_year_built', 'descrip_lot_sqft', 'descrip_sqft',\n",
      "       'descrip_beds', 'descrip_baths', 'property_type_condo',\n",
      "       'property_type_single_family', 'pets_allowed', 'state_target_encoded',\n",
      "       'city_target_encoded'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Choose a model for RFE\n",
    "model = LinearRegression()\n",
    "rfe = RFE(estimator=model, n_features_to_select=10)  # Adjust number of features as needed\n",
    "rfe.fit(X_train_scaled, y_train.values)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "print(\"RFE Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selected_features\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Run forward selection\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m selected_features_forward \u001b[38;5;241m=\u001b[39m \u001b[43mforward_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward Selection Selected Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, selected_features_forward)\n",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m, in \u001b[0;36mforward_selection\u001b[1;34m(X, y, significance_level)\u001b[0m\n\u001b[0;32m      9\u001b[0m new_pval \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(index\u001b[38;5;241m=\u001b[39mremaining_features)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_column \u001b[38;5;129;01min\u001b[39;00m remaining_features:\n\u001b[1;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mOLS(y, sm\u001b[38;5;241m.\u001b[39madd_constant(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m))\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m     12\u001b[0m     new_pval[new_column] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpvalues[new_column]\n\u001b[0;32m     13\u001b[0m min_p_value \u001b[38;5;241m=\u001b[39m new_pval\u001b[38;5;241m.\u001b[39mmin()\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "def forward_selection(X, y, significance_level=0.05):\n",
    "    initial_features = X_train.columns.tolist()\n",
    "    selected_features = []\n",
    "    while len(initial_features) > 0:\n",
    "        remaining_features = list(set(initial_features) - set(selected_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(y, sm.add_constant(X[selected_features + [new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if min_p_value < significance_level:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            selected_features.append(best_feature)\n",
    "        else:\n",
    "            break\n",
    "    return selected_features\n",
    "\n",
    "# Run forward selection\n",
    "selected_features_forward = forward_selection(X_train_scaled, y_train.values)\n",
    "print(\"Forward Selection Selected Features:\", selected_features_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Selection Selected Features: ['year', 'km_driven', 'kmpl', 'engine_cc', 'power_bhp', 'seats']\n"
     ]
    }
   ],
   "source": [
    "def backward_selection(X, y, significance_level=0.05):\n",
    "    features = X.columns.tolist()\n",
    "    while len(features) > 0:\n",
    "        model = sm.OLS(y, sm.add_constant(X[features])).fit()\n",
    "        p_values = model.pvalues.iloc[1:]  # Exclude intercept p-value\n",
    "        max_p_value = p_values.max()\n",
    "        if max_p_value >= significance_level:\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "        else:\n",
    "            break\n",
    "    return features\n",
    "\n",
    "# Run backward selection\n",
    "selected_features_backward = backward_selection(X, y)\n",
    "print(\"Backward Selection Selected Features:\", selected_features_backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
